{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отчет по лабораторной работе #4\n",
    "### Выполнила: Анна Казакова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследование проводилось над выборками, содержащимися в библиотеке Keras: imdb и boston_housing для классификации и регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание выборок:\n",
    "#### IMDB: \n",
    "* Множество состоит из 50 000 самых разных отзывов к кинолентам. Набор разбит на 25 000 обучающих и 25 000 контрольных отзывов, каждый набор на 50% состоит из отрицательных и на 50 % из положительных отзывов.\n",
    "* Отзывы уже преобразованы в последовательности целых чисел, каждое из которых определяет позицию слова в словаре. \n",
    "* При обучении модели использовал только 5000 самых часто встречающихся отзывов из-за ограничений железа.\n",
    "\n",
    "#### Boston Housing: \n",
    "* Содержит относительно немного образцов данных: всего 506, разбитых на 404 обучающих и 102 контрольных образца\n",
    "* Каждый признак во входных данных имеет свой масштаб\n",
    "* Содержит 13 числовых признаков\n",
    "* Цены в основной массе находятся в диапазоне от 10 000 до 50 000 долларов США"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для построения классификатора нам понадобятся следующие библиотеки и их объекты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import layers, models\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вычисления значений recall и precision были описаны данные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция кодирует последовательности целых чисел в бинарную матрицу для последующей передачи в нейронную сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=5000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим количество используемых слов, разделим выборку на тренировочные и тестовые части и преобразуем данные перед подачей в нейронную сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 5000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
    "X_train = vectorize_sequences(train_data)\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "X_test = vectorize_sequences(test_data)\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С нашей задачей прекрасно справится сеть, организованные как простой стек полносвязных слоев с операцией активации relu. Промежуточные слои будут использовать операцию relu в качестве функции активации, а последний слой будет использовать сигмоидную функцию активации и выводить вероятность. Функция relu используется для преобразования отрицательных значений в ноль. Так как перед нами стоит задача бинарной классификации и результатом работы сети является вероятность, предпочтительнее использовать функцию потерь binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(num_words,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc', recall, precision])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим проверочный набор, выбрав 5000 образцов из\n",
    "оригинального набора обучающих данных. Теперь проведем обучение модели в течение 20 эпох пакетами по 512 образцов. В то же время будем следить за потерями и точностью на 5000 отложенных образцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.4744 - acc: 0.7952 - recall: 0.7895 - precision: 0.8160 - val_loss: 0.3419 - val_acc: 0.8750 - val_recall: 0.8771 - val_precision: 0.8768\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 1s 71us/step - loss: 0.2892 - acc: 0.8954 - recall: 0.9026 - precision: 0.8918 - val_loss: 0.3010 - val_acc: 0.8788 - val_recall: 0.8395 - val_precision: 0.9146\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 1s 72us/step - loss: 0.2366 - acc: 0.9133 - recall: 0.9211 - precision: 0.9075 - val_loss: 0.3139 - val_acc: 0.8720 - val_recall: 0.8110 - val_precision: 0.9288\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 1s 70us/step - loss: 0.2076 - acc: 0.9232 - recall: 0.9271 - precision: 0.9213 - val_loss: 0.2969 - val_acc: 0.8852 - val_recall: 0.9386 - val_precision: 0.8504\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 1s 73us/step - loss: 0.1941 - acc: 0.9287 - recall: 0.9328 - precision: 0.9261 - val_loss: 0.2929 - val_acc: 0.8808 - val_recall: 0.8617 - val_precision: 0.8992\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 1s 73us/step - loss: 0.1804 - acc: 0.9312 - recall: 0.9293 - precision: 0.9309 - val_loss: 0.3050 - val_acc: 0.8836 - val_recall: 0.9227 - val_precision: 0.8583\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 1s 73us/step - loss: 0.1664 - acc: 0.9384 - recall: 0.9433 - precision: 0.9345 - val_loss: 0.3683 - val_acc: 0.8560 - val_recall: 0.7816 - val_precision: 0.9240\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 78us/step - loss: 0.1598 - acc: 0.9404 - recall: 0.9416 - precision: 0.9399 - val_loss: 0.3173 - val_acc: 0.8824 - val_recall: 0.8836 - val_precision: 0.8849\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 76us/step - loss: 0.1512 - acc: 0.9441 - recall: 0.9477 - precision: 0.9430 - val_loss: 0.3561 - val_acc: 0.8682 - val_recall: 0.8214 - val_precision: 0.9108\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 77us/step - loss: 0.1416 - acc: 0.9486 - recall: 0.9505 - precision: 0.9497 - val_loss: 0.3799 - val_acc: 0.8662 - val_recall: 0.8114 - val_precision: 0.9165\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 2s 77us/step - loss: 0.1385 - acc: 0.9474 - recall: 0.9429 - precision: 0.9491 - val_loss: 0.3595 - val_acc: 0.8766 - val_recall: 0.9129 - val_precision: 0.8543\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 1s 75us/step - loss: 0.1316 - acc: 0.9512 - recall: 0.9524 - precision: 0.9513 - val_loss: 0.3730 - val_acc: 0.8738 - val_recall: 0.9133 - val_precision: 0.8495\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 1s 73us/step - loss: 0.1249 - acc: 0.9526 - recall: 0.9529 - precision: 0.9513 - val_loss: 0.3779 - val_acc: 0.8744 - val_recall: 0.8731 - val_precision: 0.8788\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 2s 79us/step - loss: 0.1206 - acc: 0.9535 - recall: 0.9551 - precision: 0.9548 - val_loss: 0.4015 - val_acc: 0.8694 - val_recall: 0.8429 - val_precision: 0.8945\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 1s 72us/step - loss: 0.1156 - acc: 0.9574 - recall: 0.9577 - precision: 0.9570 - val_loss: 0.4770 - val_acc: 0.8546 - val_recall: 0.7858 - val_precision: 0.9177\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 1s 72us/step - loss: 0.1085 - acc: 0.9592 - recall: 0.9553 - precision: 0.9614 - val_loss: 0.4188 - val_acc: 0.8712 - val_recall: 0.8642 - val_precision: 0.8803\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 1s 72us/step - loss: 0.1040 - acc: 0.9613 - recall: 0.9602 - precision: 0.9587 - val_loss: 0.4394 - val_acc: 0.8678 - val_recall: 0.8479 - val_precision: 0.8873\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 2s 82us/step - loss: 0.1004 - acc: 0.9622 - recall: 0.9618 - precision: 0.9634 - val_loss: 0.4449 - val_acc: 0.8702 - val_recall: 0.8625 - val_precision: 0.8799\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 1s 75us/step - loss: 0.0924 - acc: 0.9663 - recall: 0.9634 - precision: 0.9665 - val_loss: 0.4608 - val_acc: 0.8682 - val_recall: 0.8912 - val_precision: 0.8556\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 2s 80us/step - loss: 0.0906 - acc: 0.9665 - recall: 0.9617 - precision: 0.9665 - val_loss: 0.5056 - val_acc: 0.8580 - val_recall: 0.9152 - val_precision: 0.8246\n"
     ]
    }
   ],
   "source": [
    "x_val = X_train[:num_words]\n",
    "partial_x_train = X_train[num_words:]\n",
    "y_val = y_train[:num_words]\n",
    "partial_y_train = y_train[num_words:]\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))\n",
    "history_dict = history.history\n",
    "epochs = range(1, len(history_dict['acc'])+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что на этапе обучения потери снижаются с каждой эпохой, а точность растет. Но это не относится к потерям и точности на этапе проверки: похоже, что они достигли пика в четвертую эпоху. Соответственно можно утверждать, что в данном случае наблюдается переобучение. В данном случае для предотвращения переобучения можно прекратить обучение после третьей эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f2bcdab55b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_recall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(epochs, history_dict['recall'], 'bo', label='recall')\n",
    "plt.plot(epochs, history_dict['val_recall'], 'b', label='val_recall')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('recall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(epochs, history_dict['precision'], 'bo', label='precision')\n",
    "plt.plot(epochs, history_dict['val_precision'], 'b', label='val_precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(epochs, history_dict['acc'], 'bo', label='accuracy')\n",
    "plt.plot(epochs, history_dict['val_acc'], 'b', label='val_accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим новую сеть с нуля в течение четырех эпох и затем оценим получившийся результат на контрольных данных. В итоге мы достигли точности в ~88%, что является хорошим результатов для такой простой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(num_words,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=4, batch_size=512)\n",
    "model.save('imdb.h5')\n",
    "print(model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь перейдем к задаче регрессии. Библиотеки и их объекты, которые понадобятся нам для построения этой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import boston_housing\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализуем данные перед передачей в нейронную сеть: для каждого признака во входных данных из каждого значения вычитается среднее по\n",
    "этому признаку, и разность делится на стандартное отклонение, в результате признак центрируется по нулевому значению и имеет стандартное отклонение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее опишем функцию построения нашей сети: она заканчивается одномерным слоем, не имеющим функции активации. Это типичная конфигурация для скалярной регрессии. Функция mse широко используется\n",
    "в задачах регрессии. Также мы включили мониторинг на абсолютное значение разности между предсказанными и целевыми значениями (mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как размерность нашего набора слишком мала для разбивки на обучающий и проверочный наборы лучшей практикой в таких ситуациях является применение перекрестной проверки по K блокам. В данном случае\n",
    "средняя ошибка составила 3000 долларов, что довольно много"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate([train_data[:i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0)\n",
    "    partial_train_targets = np.concatenate([train_targets[:i * num_val_samples], train_targets[(i + 1) * num_val_samples:]], axis=0)\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)\n",
    "print(np.mean(all_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем увеличить время обучения сети до 500 эпох. Чтобы получить информацию о качестве обучения модели в каждую эпоху, изменим цикл обучения и добавим сохранение оценки проверки перед началом эпохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate([train_data[:i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0)\n",
    "    partial_train_targets = np.concatenate([train_targets[:i * num_val_samples], train_targets[(i + 1) * num_val_samples:]], axis=0)\n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data, partial_train_targets, validation_data=(val_data, val_targets), epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    mae_history = history.history['val_mae']\n",
    "    all_mae_histories.append(mae_history)\n",
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график mae, исключая первые 10 замеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим окончательную версию модели и получим результат нашей работы. К сожалению средняя ошибка все еще составляет около 2550 долларов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.fit(train_data, train_targets, epochs=80, batch_size=16, verbose=0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
    "print(test_mse_score, test_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "В данных примерах да и в целом перед построением и обучением моделей на каких-либо данных существует необходимость их преобразования в тензоры - векторы числовых данных признаков (в задаче регрессии их пришлось нормализовать). Также мы убедились, что по мере улучшения на обучающих данных нейронные сети рано или поздно\n",
    "начинают переобучаться, демонстрируя ухудшение результатов на данных, которые они прежде не видели. За этим нужно следить и использовать известные методы борьбы с этим. Во избежание переобучения при небольших объемах данных следует использовать небольшие сети. При небольшом объемы данных как в нашей задаче регрессии для надежной оценки качества модели следует использовать метод перекрестной проверки по K блокам"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
